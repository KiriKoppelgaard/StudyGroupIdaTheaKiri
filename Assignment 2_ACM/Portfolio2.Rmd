---
title: "Portfolio2"
author: "KK"
date: '2022-03-09'
output: html_document
---
During this assignment you wrap up the last few weeks of model exploration. You pick the model you have been working on and write a report:

- describing the model (you can re-use text from assignment 1, if relevant) 
We here take a closer look at the win-stay-lose-shift model.

- showcasing a commented version of the stan model (what does each line do?) 

- describing and motivating a process of parameter recovery (why are you doing it?, how are you doing it?) (nr of trials and combi of parameter values)

- report parameter recovery

- discussing the results: how many trials should be used at least to properly recover the parameters? Add relevant plot(s).

#Load libraries and data for a single agent to do sensitivity checks
```{r}
library(pacman)
p_load(tidyverse, here, posterior, cmdstanr, boot, brms)

#load data, specify the biases
single_agent <- na.omit(read.csv('data/simulated_data.csv') %>% filter(leavebias==0.5 & staybias ==0.5))
single_agent <- single_agent[1:500,]

```

#Sensitivity check
Create prior values to loop trough during sensitivity check. 
```{r}

#Create priors 
prior_mean_bstay <- seq(-3, 3, .5)
prior_sd_bstay <- seq(0.1, 1, 0.1)

#Create all possible combinations 
priors <- expand.grid(prior_mean_bstay, prior_sd_bstay)

#reshape the data
priors <- tibble(prior_mean_bstay = priors$Var1, prior_sd_bstay=priors$Var2)

```


#STAN model
Define the STAN-model of WSLS. 
```{r}
stan_file_single <- write_stan_file("
// This Stan model infers a rate (theta) from a number of trials (n) and successes (k)

// The input data is two integer numbers: n and k.
data {
  int <lower=1> n; // n of trials 
  array[n]int k;  // choices
  vector<lower=-1, upper=1>[n] win; //stay bias
  vector<lower=-1, upper=1>[n] lose; //leave bias
  real prior_mean_bstay; 
  real <lower = 0> prior_sd_bstay;
}

// The parameters accepted by the model. Our model accepts only theta, the rate, 
// which is bound at 0 (no chances of success) and 1 (always success)
parameters {
  real alpha;
  real beta_stay;
  real beta_leave;
}

transformed parameters{
  vector[n] theta;
  theta = alpha + (beta_stay*win)+ (beta_leave*lose); //multiplying the paramaters we want to estimate on the data
}

// The model to be estimated; prior and likelihood
model {
  // The prior for theta is a uniform distribution between 0 and 1
  target += normal_lpdf(alpha | 0, 1);
  target += normal_lpdf(beta_stay | prior_mean_bstay, prior_sd_bstay);
  target += normal_lpdf(beta_leave | 0, 0.3);

  // The model consists in a binomial distribution with a rate theta, 
  // and a number of trials n generating k successes
  target += bernoulli_logit_lpmf(k | theta);
}

generated quantities{
  real bstay_prior;
  real bstay_posterior;
  int<lower=0, upper=n> prior_preds;
  array[n] int <lower=0, upper=n> posterior_preds;

  bstay_prior = normal_rng(0,1);
  bstay_posterior = beta_stay;
  
  prior_preds = binomial_rng(n, inv_logit(bstay_prior));
  posterior_preds = binomial_rng(n, inv_logit(theta));
}

")
```


```{r}
#Compile the model
mod_single <- cmdstan_model(stan_file_single, cpp_options = list(stan_threads = TRUE), pedantic = TRUE)

```

Technically you could access parameter values all four possible conditions as follows: 

- posteriorWH_preds = binomial_rng(n, inv_logit(alpha + beta_stay*1 + beta_leave*0));

#Draw posterior distributions
```{r}
sensitivity_df <- NULL
trials_n <- nrow(single_agent)

for (p in seq(nrow(priors))){
  data <- list(
    n = trials_n, 
    k = single_agent$Self[1:trials_n],
    win = single_agent$win[1:trials_n],
    lose = single_agent$lose[1:trials_n],
    prior_mean_bstay = priors$prior_mean_bstay[p],
    prior_sd_bstay = priors$prior_sd_bstay[p]
  )
  
  samples <- mod_single$sample(
  data = data,
  seed = 123,
  chains = 2,
  parallel_chains = 2,
  threads_per_chain = 2,
  iter_warmup = 500,
  iter_sampling = 500,
  refresh = 500,
  max_treedepth = 15,
  adapt_delta = 0.99)
  
  draws_df <- as_draws_df(samples$draws())
  
  temp <- tibble(bstay_prior = draws_df$bstay_prior, 
                 bstay_posterior = draws_df$bstay_posterior,
                 prior_preds = draws_df$prior_preds, 
                 posterior_preds = draws_df$posterior_preds, 
                 prior_mean_bstay = priors$prior_mean_bstay[p],
                 prior_sd_bstay = priors$prior_sd_bstay[p]
                 )
  if(exists("sensitivity_df")){sensitivity_df <- rbind (sensitivity_df, temp)} else {sensitivity_df <- temp}
  
}
```


#Prior sensitivity plot
```{r}
#Plot of mean
ggplot(data=sensitivity_df, aes(x=prior_mean_bstay, y=inv.logit(bstay_posterior))) +
  facet_wrap(~ prior_sd_bstay) + 
  geom_point(size = 3, alpha = .3) +
  geom_hline(yintercept = 0.9, color = 'red', linetype = 'dashed') + 
  labs(x="Mean", y="Posterior", title = 'Prior Sensitivity Check for Beta Stay')+ 
  theme_bw()

#Plot of sd
ggplot(data=sensitivity_df, aes(x=prior_sd_bstay, y=inv.logit(bstay_posterior))) +
  facet_wrap(~ prior_mean_bstay) + 
  geom_point(size = 3, alpha = .3) +
  geom_hline(yintercept = 0.9, color = 'red', linetype = 'dashed') + 
  labs(x="SD", y="Posterior", title = 'Prior Sensitivity Check for Beta Stay')+ 
  theme_bw()
  
```


#Plots of prior and for predictive checks

```{r}
#Prior predictive check 
ggplot(draws_df) + 
  geom_histogram(aes(prior_preds),
  color="darkblue", fill="blue", alpha=0.3) + xlab("Predicted heads out of 120 trials") + 
  ylab("Posterior Density") + theme_classic()

#Has the model learned from the data? 
ggplot(draws_df) +
  geom_density(aes(bstay_posterior), fill="blue", alpha=0.3) + 
  geom_density(aes(bstay_prior), fill="red", alpha=0.3) +   xlab("Rate") +
  ylab("Posterior Density") +
  theme_classic()
# 
# #Are the predictions alright?
# ggplot(draws_df) + geom_histogram(aes(posterior_preds), color="darkblue", fill="blue", alpha=0.3, bins=90) +
#   geom_point(x = sum(data$k), y = 0, color="red", shape = 17, size = 5) +
#   xlab("Predicted heads out of 120 trials") + ylab("Posterior Density") + theme_classic()
# 
# #Learning in the predictions
# ggplot(draws_df) +
#   geom_histogram(aes(prior_preds), color="lightblue", fill="blue", alpha=0.3, bins=90) +
#   geom_histogram(aes(posterior_preds), color="darkblue", fill="blue", alpha=0.3, bins=90) +
#   geom_point(x = sum(df$Self[0:120]), y = 0, color="red", shape = 17, size = 5) +
#   xlab("Predicted heads out of 120 trials") + ylab("Density") +
#   theme_classic()

```


#Perform parameter recovery with all combinations of leavebias and staybias
```{r}
#Load simulated data with different rates
recovery_trials_df <- read.csv('data/simulated_data.csv')

#removing NAs
recovery_trials_df <- na.omit(recovery_trials_df)

#checking that they work as intended
y <- recovery_trials_df %>% group_by(staybias, win) %>% dplyr::summarise(rate=mean(Self))
ggplot(na.omit(y), aes(staybias,rate)) +geom_point() +facet_grid(.~win)
```


```{r}
stan_file <- write_stan_file("
// This Stan model infers two rates; a StayBias and a LeaveBias to model a WSLS agent

// The input data is 
data {
  int <lower=1> n; // n of trials 
  array[n]int k;  // choices; 0 = tails, 1 = heads
  vector<lower=-1, upper=1>[n] win; // feedback, if win and heads = 1, win and tails = -1
  vector<lower=-1, upper=1>[n] lose; //feedback, if lose and heads = -1, lose and tails = 1
}

// The parameters accepted by the model. Our model accepts only theta, the rate, 
// which is bound at 0 (no chances of success) and 1 (always success)
parameters {
  real alpha; // intercept
  real beta_stay; // the propensity to stay, given the agent won
  real beta_leave; //the propensity to leave, given the agent lost
}

transformed parameters{
  vector[n] theta; // theta is a vector of same length as n trials
  theta = alpha + (beta_stay*win) + (beta_leave*lose); // the model
}

// The model to be estimated; prior and likelihood
model {
  // The prior for theta is a normal distribution between 0 and 1
  target += normal_lpdf(alpha | 0, 1); // prior for alpha is a normal distribution with a mean of 0 and sd of 1. Hereby, we assume that there is equal chance of choosing heads or tails regardless of anything else  
  target += normal_lpdf(beta_stay | 0, 0.3); // setting prior for StayBias
  target += normal_lpdf(beta_leave | 0, 0.3); // setting prior for LeaveBias

  // The model 
  target += bernoulli_logit_lpmf(k | theta); // the data given theta aka the model
}
// This block quantifies the 
generated quantities{
  real leavebias_posterior;
  real staybias_posterior;

  staybias_posterior = beta_stay;
  leavebias_posterior = beta_leave;
  
}

")

```

```{r}
#Compile the model
mod <- cmdstan_model(stan_file, cpp_options = list(stan_threads = TRUE), pedantic = TRUE)
```

# Parameter recovery: LeaveBias

```{r}
#maintain constant staybias
df_full <- recovery_trials_df[recovery_trials_df$staybias == 0.7,]

for (leavebias in unique(df_full$leavebias)){
  for (trial_n in c(10, 25, 50, 100, 200, 500, 1000, 2500, 4900)){
    df <- df_full[df_full$leavebias == leavebias, ] 
    
    data <- list(
      n = as.numeric(trial_n-1), 
      k = df$Self[2:trial_n],
      win = df$win[2:trial_n],
      lose = df$lose[2:trial_n]
    )
    
    samples <- mod$sample(
    data = data,
    seed = 123,
    chains = 2,
    parallel_chains = 2,
    threads_per_chain = 2,
    iter_warmup = 1000,
    iter_sampling = 1000,
    refresh = 1000,
    max_treedepth = 20,
    adapt_delta = 0.99)
    
    draws_df <- as_draws_df(samples$draws())
    
    temp <- tibble(leavebias_true = leavebias, 
                   trials = trial_n, 
                   staybias_true = unique(df_full$staybias),
                   leavebias_posterior = draws_df$leavebias_posterior
                   )
    if(exists("param_leave_df")){param_leave_df <- rbind (param_leave_df, temp)} else {param_leave_df <- temp}
  }  
}

```

```{r}
#Plot of leave bias
ggplot(data=param_leave_df, aes(x=leavebias_true, y=inv.logit(leavebias_posterior))) +
  facet_wrap(~ trials) + 
  geom_point(size = 3, alpha = .3) +
  geom_abline(intercept = 0.5, slope =0.5, color = 'red',linetype="dashed") +
  stat_summary(fun=mean,color = 'blue', geom = 'line', aes(group = 1)) + 
  labs(x="True Parameter", y="Estimated Parameter", title = 'Parameter Recovery: LeaveBias')+ 
  theme_bw()

```

#Parameter recovery: StayBias

```{r}
#maintain constant leavebias
df_full <- recovery_trials_df[recovery_trials_df$leavebias == 0.7,]

#loop trough different staybias
for (staybias in unique(df_full$staybias)){
  for (trial_n in c(10, 25, 50, 100, 200, 500, 1000, 2500, 4900)){
    df <- df_full[df_full$staybias == staybias, ] 
    
    data <- list(
      n = as.numeric(trial_n-1), 
      k = df$Self[2:trial_n],
      win = df$win[2:trial_n],
      lose = df$lose[2:trial_n]
    )
    
    samples <- mod$sample(
    data = data,
    seed = 123,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 1000,
    iter_sampling = 1000,
    refresh = 1000,
    max_treedepth = 20,
    adapt_delta = 0.99)
    
    draws_df <- as_draws_df(samples$draws())
    
    temp <- tibble(staybias_true = staybias, 
                   trials = trial_n, 
                   leavebias_true = unique(df_full$leavebias),
                   staybias_posterior = draws_df$staybias_posterior
                   )
    if(exists("param_stay_df")){param_stay_df <- rbind (param_stay_df, temp)} else {param_stay_df <- temp}
  }  
}

```

```{r}
#Plot of staybias parameter recovery
ggplot(data=param_stay_df, aes(x=staybias_true, y=inv.logit(staybias_posterior))) +
  facet_wrap(~ trials) + 
  geom_point(size = 3, alpha = .3) +
  geom_abline(intercept = 0.5, slope =0.5, color = 'red',linetype="dashed") + 
  stat_summary(fun=mean,color = 'blue', geom = 'line', aes(group = 1)) + 
  labs(x="True Parameter", y="Estimated Parameter", title = 'Parameter Recovery: StayBias')+ 
  theme_bw()

```


#StayBias and LeaveBias: Parameter Recovery

```{r}
#loop trough different staybias
for (staybias in unique(recovery_trials_df$staybias)){
  for (leavebias in unique(recovery_trials_df$leavebias)){
    df <- recovery_trials_df[recovery_trials_df$staybias == staybias & recovery_trials_df$leavebias == leavebias, ] 
    
    data <- list(
      n = 120, 
      k = df$Self[2:120],
      win = df$win[2:120],
      lose = df$lose[2:120]
    )
    
    samples <- mod$sample(
    data = data,
    seed = 123,
    chains = 1,
    parallel_chains = 1,
    threads_per_chain = 1,
    iter_warmup = 1000,
    iter_sampling = 1000,
    refresh = 1000,
    max_treedepth = 20,
    adapt_delta = 0.99)
    
    draws_df <- as_draws_df(samples$draws())
    
    temp <- tibble(staybias_true = staybias, 
                   leavebias_true = leavebias,
                   staybias_posterior = draws_df$staybias_posterior,
                   leavebias_posterior = draws_df$leavebias_posterior, 
                   trials = 120
                   )
    if(exists("param_stay_leave_df")){param_stay_leave_df <- rbind (param_stay_leave_df, temp)} else {param_stay_leave_df <- temp}
  }  
}

```

```{r}
#Plot of StayBias and LeaveBias
ggplot(data=param_stay_leave_df, aes(x=staybias_true, y=inv.logit(staybias_posterior))) +
  facet_wrap(~ leavebias_true) + 
  geom_point(size = 3, alpha = .3) +
  geom_line(data = param_stay_leave_df, aes(y = staybias_true), color = 'red') + 
  stat_summary(fun=mean,color = 'blue', geom = 'line', aes(group = 1)) + 
  labs(x="True Parameter: StayBias", y="Estimated Parameter: StayBias", title = 'Parameter Recovery: StayBias and LeaveBias')+ 
  theme_bw()

#Plot of StayBias and LeaveBias
ggplot(data=param_stay_leave_df, aes(x=leavebias_true, y=inv.logit(leavebias_posterior))) +
  facet_wrap(~ staybias_true) + 
  geom_point(size = 3, alpha = .3) +
  geom_line(data = param_stay_leave_df, aes(y = leavebias_true), color = 'red') + 
  stat_summary(fun=mean,color = 'blue', geom = 'line', aes(group = 1)) + 
  labs(x="True Parameter: LeaveBias", y="Estimated Parameter:LeaveBias", title = 'Parameter Recovery: StayBias and LeaveBias')+ 
  theme_bw()



```
