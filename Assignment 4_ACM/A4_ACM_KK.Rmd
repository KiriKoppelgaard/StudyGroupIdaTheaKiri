---
title: "A4_ACM"
output: html_document
date: '2022-04-22'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#libraries
library(pacman)
p_load(tidyverse, here, posterior, cmdstanr, boot, brms)

```

# Part 1

You have to design a study (aka plan the number of trials) assuming that people are using a reinforcement learning process to pick between 2 stimuli. In this study you expect there to be 2 conditions and the 1 participant playing the game will vary its learning rate between conditions. 

The difference in learning rate is .2: 

condition 1: x - .1
condition 2: x + .1

with x = 0.7. 

The temperature (tau) is the same: 0.5.
Identify a feasible number of trials and motivate it.

## Simulating data

### Functions 
```{r}
#Creating softmax function to select choice among more options
softmax <- function(x,tau) {
  outcome = 1/(1+exp(-tau*x))
  return(outcome) #the choice that is made
}

#Create update function, that simulates reinforcement learning
ValueUpdate = function(value, alpha, choice, feedback){
  #predicted reward + lr * prediction error
  v1 <- value[1] + alpha * (1-choice) * (feedback - value[1]) 
  v2 <- value[2] + alpha * (choice) * (feedback - value[2])
  updatedValue <- c(v1, v2)
  
  #value = expected reward, feedback = reward, choice = slot machine
}

```


###Defining variables and preparing data frame
```{r}
#defining intial values
value <- c(0, 0) 
alpha <- c(0.6, 0.8)
temperature <- 0.5
choice <- 0
feedback <- -1
conditions = 2
p <- 0.75
trials = 1000

#Intialising data frame
d <- tibble(choice = rep(NA, trials),
		value1 = rep(NA, trials), 
		value2 = rep(NA, trials),
		feedback = rep(NA, trials), 
		condition = rep(NA, trials), 
		trial = rep(NA, trials))

d2 <- tibble(choice = rep(NA, trials),
		value1 = rep(NA, trials), 
		value2 = rep(NA, trials),
		feedback = rep(NA, trials), 
		condition = rep(NA, trials), 
		trial = rep(NA, trials))

```


###Simulating data 
```{r}
#Define the bot mechanism a.k.a. the two arm bandit
Bot <- rbinom(trials, 1, p)

#Condition 1
#Loop trough conditions and trials and save values
current_alpha <- alpha[1] #save lr for current condition
for(i in 1:trials){
	choice <- rbinom(1, 1 ,softmax(value[2] - value[1], temperature))
	feedback <- ifelse(Bot[i] == choice, 1, -1)
	value <- ValueUpdate(value, current_alpha, choice, feedback)
	d$choice[i] <- choice
	d$value1[i] <- value[1]
	d$value2[i] <- value[2]
	d$feedback[i] <- feedback
	d$condition[i] <- current_alpha
	d$trial[i] <- i
}
#Condition 2
#reset values
choice <- 0
feedback <- -1
value <- c(0, 0) 

#Loop trough conditions and trials and save values
current_alpha <- alpha[2] #save lr for current condition
for(i in 1:trials){
	choice <- rbinom(1, 1 ,softmax(value[2] - value[1], temperature))
	feedback <- ifelse(Bot[i] == choice, 1, -1)
	value <- ValueUpdate(value, current_alpha, choice, feedback)
	d2$choice[i] <- choice
	d2$value1[i] <- value[1]
	d2$value2[i] <- value[2]
	d2$feedback[i] <- feedback
	d2$condition[i] <- current_alpha
	d2$trial[i] <- i
}

#Rbind the two data frames
d <- rbind(d, d2)  

```

```{r}
data <- list(
      trials = trials, 
      feedback = d$feedback,
      choice = d$choice,
      condition1 = ifelse(d$condition == 0.8, 1, 0),
      condition2 = 1 - ifelse(d$condition == 0.8, 1, 0)
          )
```


[optional]: what happens if x is not = +.7 (tip: test a range of different x)?
[optional]: what happens if temperature is not 0.5, but 5?

# Part 2
Given the large number of trials required, could you imagine producing an iterated design? E.g. a phone app where you can do a smaller number of trials (e.g. 10-20 or even 100, up to you!) in separate sessions, each time a posterior is generated and it is used as prior in the next time.
Assuming no variance over time (ah!) can you figure out a good trade off between how many trials per session and number of sessions?






[optional]: what are the differences in just re-running the model on the cumulative dataset (increased at every session) vs passing the posterior? Differences in terms of computational time, estimates, but also practical implication for running your study.
[optional]: what happens if learning rate changes a bit across sessions? Include a variation between sessions according to a normal distribution with a mean of 0 and a sd of 0.02. Re-assess the number of trials/sessions used.


